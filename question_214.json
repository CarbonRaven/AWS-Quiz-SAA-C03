{
  "id": 214,
  "topic": "Topic 1",
  "question": "A company's reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company must convert these files to Apache Parquet format and must store the files in a transformed data bucket.\nWhich solution will meet these requirements with the LEAST development effort?",
  "options": {
    "A": "Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to transform the data. Use EMR File System (EMRFS) to write files to the transformed data bucket.",
    "B": "Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step.",
    "C": "Use AWS Batch to create a job definition with Bash syntax to transform the data and output the data to the transformed data bucket. Use the job definition to submit a job. Specify an array job as the job type.",
    "D": "Create an AWS Lambda function to transform the data and output the data to the transformed data bucket. Configure an event notification for the S3 bucket. Specify the Lambda function as the destination for the event notification."
  },
  "correct_answer": "B",
  "explanation": "AWS Glue provides built-in support for CSV to Parquet conversion with minimal code. Glue crawlers auto-discover schema, and Glue ETL jobs can be created visually or with simple scripts. This requires the least development effort compared to EMR Spark or custom Lambda code."
}
