{
  "id": 956,
  "topic": "Topic 1",
  "question": "A company is migrating its data processing application to the AWS Cloud. The application processes several short-lived batch jobs that cannot be disrupted. Data is generated after each batch job is completed. The data is accessed for 30 days and retained for 2 years.\n\nThe company wants to keep the cost of running the application in the AWS Cloud as low as possible.\n\nWhich solution will meet these requirements?",
  "options": {
    "A": "Migrate the data processing application to Amazon EC2 Spot Instances. Store the data in Amazon S3 Standard. Move the data to Amazon S3 Glacier Instant. Retrieval after 30 days. Set an expiration to delete the data after 2 years.",
    "B": "Migrate the data processing application to Amazon EC2 On-Demand Instances. Store the data in Amazon S3 Glacier Instant Retrieval. Move the data to S3 Glacier Deep Archive after 30 days. Set an expiration to delete the data after 2 years.",
    "C": "Deploy Amazon EC2 Spot Instances to run the batch jobs. Store the data in Amazon S3 Standard. Move the data to Amazon S3 Glacier Flexible Retrieval after 30 days. Set an expiration to delete the data after 2 years.",
    "D": "Deploy Amazon EC2 On-Demand Instances to run the batch jobs. Store the data in Amazon S3 Standard. Move the data to Amazon S3 Glacier Deep Archive after 30 days. Set an expiration to delete the data after 2 years."
  },
  "correct_answer": "D",
  "explanation": "Since batch jobs cannot be disrupted, On-Demand Instances are required (Spot can be interrupted). S3 Standard for 30 days of active access, then Glacier Deep Archive for long-term retention (2 years) provides the most cost-effective storage solution."
}
