{
  "id": 328,
  "topic": "Topic 1",
  "question": "A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously.\n\nThe company is expecting a significant and sudden increase in the number of sales requests during events for the launch of new products.\n\nWhat should a solutions architect recommend to ensure that all the requests are processed successfully?",
  "options": {
    "A": "Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in traffic.",
    "B": "Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.",
    "C": "Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce traffic for the API to handle.",
    "D": "Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances."
  },
  "correct_answer": "D",
  "explanation": "CloudFront for static content reduces load on the API. SQS queue buffers sales requests ensuring none are lost during traffic spikes, allowing backend workers to process asynchronously. This ensures all requests are processed successfully. Static manual instance increases can't handle sudden spikes. ElastiCache doesn't queue requests."
}
