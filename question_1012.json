{
  "id": 1012,
  "topic": "Topic 1",
  "question": "A company has a three-tier web application that processes orders from customers. The web tier consists of Amazon EC2 instances behind an Application Load Balancer. The processing tier consists of EC2 instances. The company decoupled the web tier and processing tier by using Amazon Simple Queue Service (Amazon SQS). The storage layer uses Amazon DynamoDB.\n\nAt peak times, some users report order processing delays and halts. The company has noticed that during these delays, the EC2 instances are running at 100% CPU usage, and the SQS queue fills up. The peak times are variable and unpredictable.\n\nThe company needs to improve the performance of the application.\n\nWhich solution will meet these requirements?",
  "options": {
    "A": "Use scheduled scaling for Amazon EC2 Auto Scaling to scale out the processing tier instances for the duration of peak usage times. Use the CPU Utilization metric to determine when to scale.",
    "B": "Use Amazon ElastiCache for Redis in front of the DynamoDB backend tier. Use target utilization as a metric to determine when to scale.",
    "C": "Add an Amazon CloudFront distribution to cache the responses for the web tier. Use HTTP latency as a metric to determine when to scale.",
    "D": "Use an Amazon EC2 Auto Scaling target tracking policy to scale out the processing tier instances. Use the ApproximateNumberOfMessages attribute to determine when to scale."
  },
  "correct_answer": "D",
  "explanation": "Since the SQS queue fills up and processing EC2 instances hit 100% CPU during unpredictable peak times, the solution is to scale the processing tier based on queue depth. Using ApproximateNumberOfMessages with Auto Scaling target tracking directly addresses the backlog. Scheduled scaling won't work for unpredictable peaks."
}
